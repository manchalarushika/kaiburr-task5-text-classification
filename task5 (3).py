# -*- coding: utf-8 -*-
"""task5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uUzK0qHVMMiQDUDfrqZi-lTyRIEAnJmK
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import re
import nltk
from nltk.corpus import stopwords
import multiprocessing as mp

nltk.download('stopwords')
stop_words = set(stopwords.words('english'))

import pandas as pd

url = "https://files.consumerfinance.gov/ccdb/complaints.csv.zip"
chunksize = 100000

chunks = pd.read_csv(url, compression='zip', chunksize=chunksize, low_memory=False)
df = pd.concat(chunks, ignore_index=True)

print("Dataset loaded successfully! Shape:", df.shape)

df = df[['Product', 'Consumer complaint narrative']].dropna()
df.rename(columns={'Consumer complaint narrative': 'complaint'}, inplace=True)

categories = [
    'Credit reporting, repair, or other',
    'Debt collection',
    'Consumer Loan',
    'Mortgage'
]
df = df[df['Product'].isin(categories)].reset_index(drop=True)
print("Filtered dataset shape:", df.shape)
print(df['Product'].value_counts())

# =========================================================
# STEP 1: EXPLORATORY DATA ANALYSIS (EDA)
# =========================================================
print("\n--- Basic EDA ---")
print("Number of unique products:", df['Product'].nunique())
print("Sample complaints:\n", df['complaint'].head(3).tolist())

# Complaint length analysis
df['length'] = df['complaint'].apply(len)
print("\nAverage complaint length:", df['length'].mean())

plt.figure(figsize=(6,4))
sns.histplot(df['length'], bins=50, kde=True)
plt.title("Distribution of Complaint Text Lengths")
plt.xlabel("Complaint length (characters)")
plt.ylabel("Count")
plt.show()

plt.figure(figsize=(6,4))
sns.countplot(y='Product', data=df, order=df['Product'].value_counts().index)
plt.title("Class Distribution")
plt.xlabel("Count")
plt.ylabel("Product")
plt.show()

# =========================================================
# STEP 1: EXPLORATORY DATA ANALYSIS (EDA)
# =========================================================
print("\n--- Basic EDA ---")
print("Number of unique products:", df['Product'].nunique())
print("Sample complaints:\n", df['complaint'].head(3).tolist())

# Complaint length analysis
df['length'] = df['complaint'].apply(len)
print("\nAverage complaint length:", df['length'].mean())

plt.figure(figsize=(6,4))
sns.histplot(df['length'], bins=50, kde=True)
plt.title("Distribution of Complaint Text Lengths")
plt.xlabel("Complaint length (characters)")
plt.ylabel("Count")
plt.show()

plt.figure(figsize=(6,4))
sns.countplot(y='Product', data=df, order=df['Product'].value_counts().index)
plt.title("Class Distribution")
plt.xlabel("Count")
plt.ylabel("Product")
plt.show()

# Use only 50k rows for quick testing
df = df.sample(50000, random_state=42).reset_index(drop=True)
print("Subset for testing shape:", df.shape)

from nltk.stem import WordNetLemmatizer
nltk.download('wordnet')

lemmatizer = WordNetLemmatizer()

def clean_text(text):
    text = re.sub(r'[^a-z\s]', '', str(text).lower())
    words = [lemmatizer.lemmatize(w) for w in text.split() if w not in stop_words]
    return " ".join(words)

# Parallel cleaning using all CPU cores
def parallel_clean(texts):
    with mp.Pool(mp.cpu_count()) as pool:
        cleaned = pool.map(clean_text, texts)
    return cleaned

df['clean_text'] = parallel_clean(df['complaint'].tolist())
df.to_csv('cleaned_complaints.csv', index=False)
print("Cleaned data saved to 'cleaned_complaints.csv'")

print("Sample cleaned text:")
print(df[['Product', 'clean_text']].head())

# =========================================================
# STEP 2: WORDCLOUD VISUALIZATION (OPTIONAL)
# =========================================================
from wordcloud import WordCloud

for category in df['Product'].unique():
    text = " ".join(df[df['Product'] == category]['clean_text'])
    wc = WordCloud(width=800, height=400, background_color='white').generate(text)
    plt.figure(figsize=(8,4))
    plt.imshow(wc, interpolation='bilinear')
    plt.axis('off')
    plt.title(f"WordCloud for {category}")
    plt.show()

# =========================================================
# STEP 5: FEATURE EXTRACTION (TF-IDF)
# =========================================================
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import LabelEncoder

tfidf = TfidfVectorizer(max_features=5000, ngram_range=(1,2))

X = tfidf.fit_transform(df['clean_text'])

le = LabelEncoder()
y = le.fit_transform(df['Product'])

print("TF-IDF matrix shape:", X.shape)
print("Target classes:", list(le.classes_))

# =========================================================
# STEP 6: TRAIN/TEST SPLIT
# =========================================================
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)
print("Train size:", X_train.shape, "Test size:", X_test.shape)

# =========================================================
# STEP 7: MODEL TRAINING & COMPARISON
# =========================================================
import time
start_time = time.time()
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import LinearSVC
from sklearn.metrics import accuracy_score

models = {
    "Logistic Regression": LogisticRegression(max_iter=1000),
    "Naive Bayes": MultinomialNB(),
    "SVM": LinearSVC()
    # Random Forest skipped for speed, add if needed
}

results = {}
for name, model in models.items():
    print(f"Training {name}...")
    model.fit(X_train, y_train)
    preds = model.predict(X_test)
    acc = accuracy_score(y_test, preds)
    results[name] = acc
    print(f"{name}: {acc*100:.2f}%")


# Plot accuracies
plt.figure(figsize=(6,4))
sns.barplot(x=list(results.keys()), y=list(results.values()), palette='Greens_r')
plt.title("Model Accuracy Comparison")
plt.ylabel("Accuracy")
plt.xticks(rotation=20)
plt.show()

# =========================================================
# STEP 7.1: EXTENDED METRICS COMPARISON
# =========================================================
from sklearn.metrics import precision_score, recall_score, f1_score

print("\nDetailed Performance Metrics:")
for name, model in models.items():
    preds = model.predict(X_test)
    prec = precision_score(y_test, preds, average='weighted')
    rec = recall_score(y_test, preds, average='weighted')
    f1 = f1_score(y_test, preds, average='weighted')
    print(f"{name:20s} | Accuracy: {results[name]*100:.2f}% | Precision: {prec:.2f} | Recall: {rec:.2f} | F1: {f1:.2f}")

# =========================================================
# STEP 8: EVALUATION (CLASSIFICATION REPORT & CONFUSION MATRIX)
# =========================================================
from sklearn.metrics import classification_report, confusion_matrix

best_model = LogisticRegression(max_iter=1000)
best_model.fit(X_train, y_train)
y_pred = best_model.predict(X_test)

print("\nClassification Report:\n")
print(classification_report(y_test, y_pred, target_names=le.classes_))

cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(6,5))
sns.heatmap(cm, annot=True, fmt='d', xticklabels=le.classes_, yticklabels=le.classes_, cmap='Blues')
plt.title("Confusion Matrix - Logistic Regression",fontsize=14)
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()
# Save confusion matrix and report for submission evidence
plt.savefig("confusion_matrix.png")
with open("classification_report.txt", "w") as f:
    f.write(classification_report(y_test, y_pred, target_names=le.classes_))

# =========================================================
# STEP 9: PREDICTION DEMO & SUMMARY
# =========================================================
sample_text = ["I was wrongly charged for my mortgage payment"]
sample_clean = [clean_text(sample_text[0])]
sample_vec = tfidf.transform(sample_clean)
pred = le.inverse_transform(best_model.predict(sample_vec))

print("Sample Text:", sample_text[0])
print("Predicted Category:", pred[0])

best_model_name = max(results, key=results.get)
print("\nBest Performing Model:", best_model_name, "with Accuracy:", round(results[best_model_name]*100,2), "%")
end_time = time.time()
print(f" Total training time: {(end_time - start_time)/60:.2f} minutes")

# =========================================================
# STEP 8.1: FEATURE IMPORTANCE (INTERPRETATION)
# =========================================================
feature_names = np.array(tfidf.get_feature_names_out())
for i, class_label in enumerate(le.classes_):
    top10 = np.argsort(best_model.coef_[i])[-10:]
    print(f"\nTop features for {class_label}: {feature_names[top10]}")

print("\n" + "="*60)
print(f" Best Performing Model: {best_model_name} with Accuracy: {results[best_model_name]*100:.2f}%")
print("="*60)

import joblib
joblib.dump(best_model, 'best_model.pkl')
joblib.dump(tfidf, 'vectorizer.pkl')
print(" Model and vectorizer saved successfully!")